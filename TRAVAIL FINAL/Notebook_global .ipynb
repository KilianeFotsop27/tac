{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LA CRISE ECONOMIQUE EN BELGIQUE ENTRE 1929 ET 1933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Il est question de réaliser une étude comparative de deux journaux belges: Le Drapeau Rouge et La Libre Belgique; \n",
    "afin de voir les secteurs mis en avant pendant cette période"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I- Création de nos corpus d'analyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###A partir des fichiers extraits de la base de données CAMILLE, nous allons créer nos deux grands corpus d'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import os\n",
    "import textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Le dossier Data1 est constitué des fichiers relatifs aux articles publiés dans le journal Le Drapeau Rouge pendant ladite période (1929-1933)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path1 = '../data/Data1_Le Drapeau Rouge'\n",
    "if not os.path.exists(txt_path1):\n",
    "    os.mkdir(txt_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Le dossier Data2 quant à lui, est constitué d'articles publiés dans le journal La Libre Belgique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path2 = '../data/Data2_La libre Belgique'\n",
    "if not os.path.exists(txt_path2):\n",
    "    os.mkdir(txt_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pas important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KB_JB1051_1929-01-04_01-00002.txt',\n",
       " 'KB_JB1051_1929-01-04_01-00004.txt',\n",
       " 'KB_JB1051_1929-01-06_01-00002.txt',\n",
       " 'KB_JB1051_1929-01-09_01-00003.txt',\n",
       " 'KB_JB1051_1929-01-11_01-00003.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = []\n",
    "for f in os.listdir(txt_path1):\n",
    "    if os.path.isfile(os.path.join(txt_path1, f)):\n",
    "        txts.append(f)\n",
    "txts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KB_JB427_1929-01-26_01-00001.txt',\n",
       " 'KB_JB427_1929-01-30_01-00003.txt',\n",
       " 'KB_JB427_1929-04-12_01-00002.txt',\n",
       " 'KB_JB427_1929-06-11_01-00003.txt',\n",
       " 'KB_JB427_1929-11-06_01-00001.txt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = []\n",
    "for f in os.listdir(txt_path2):\n",
    "    if os.path.isfile(os.path.join(txt_path2, f)):\n",
    "        txts.append(f)\n",
    "txts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Création du corpus du journal Drapeau Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Data1_Le Drapeau Rouge.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(txt_path1):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(txt_path1, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Création du corpus du journal La Libre Belgique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Data2_La libre Belgique.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(txt_path2):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(txt_path2, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Nous venons ainsi de transformer les données que nous avons précedemment  extrait de la data base CAMILLE afin de constituer les corpus que nous allons ensuite exploiter tout au long de notre investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-Constatation des déchets et nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dans cette partie, il est question de débarrasser nos corpus de leurs  déchets. Pour ce faire, nous allons commencer par constater les déchets dans nos deux corpus, ensuite nous allons créer une liste de stopwords et enfin nous allons créer une fonction de nettoyage pour chacun de nos corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Importation des librairies et stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kilia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Constatation des déchets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embre 19.-'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Corpus Data1\n",
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(txt_path1, file), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1 ' ' ■ ' \""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Corpus Data2\n",
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(txt_path2, file), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Liste de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"que\", \"voici\", \"parmi\", \"chaque\", \"leurs\",\n",
    "       \"lequel\", \"alors\", \"laquelle\", \"trop\", \"fin\", \"tant\", \"ceux\", \"rien\", \"grandes\",\n",
    "       \"tÃ©l\", \"dem\", \"trÃ¨s\", \"app\", \"part\", \"ans\",\"cuis\",\"tÃ©lÃ©ph\",\"sociÃ©tÃ©\", \"rem\",\"rue\", \n",
    "       \"gar\", \"grand\", \"ecrire\", \"place\",\"monsieur\", \"quelques\", \"midi\", \"brux\", \"ecrire\", \"une\", \"sociÃ©tÃ©\",\n",
    "       \"tél\", \"ecr\", \"téléph\",\"vitae\", \"etc\", \"bur\", \"réf\", \"terr\",\"très\", \"prés\", \"déjà\", \"pers\",\n",
    "       \"effet\", \"fem\",\"également\", \"mén\", \"vers\", \"toute\", \"trav\",\"dès\", \"porte\", \"vis\", \"toujours\",\n",
    "       \"démi\", \"bel\", \"celui\", \"jour\", \"chez\", \"mod\", \"samedi\", \"madame\", \"liv\",\"tel\", \"bonne\",\n",
    "       \"bon\", \"mois\", \"temps\", \"dimanche\",\"saint\", \"bonnes\", \"peu\", \"près\", \"villa\", \"engage\", \"soir\", \"libre\",\n",
    "       \"cherche\", \"vendre\", \"car\", \"demande\", \"etat\",\"décès\", \"partie\", \"matin\", \"eau\", \"reprise\",\"rossel\", \"premier\",\n",
    "       \"première\",\"maison\", \"jours\",\"avant\",\"cependant\", \"vue\", \"but\",\"grande\", \"adresser\", \"dame\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Création d'une fonction de nettoyage pour chaque corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus Data1\n",
    "def clean_text(txt_path1, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{txt_path1}.txt\"\n",
    "        output_path = f\"{txt_path1}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{txt_path1}.txt\"\n",
    "        output_path = f\"{folder}/{txt_path1}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus Data2\n",
    "def clean_text(txt_path2, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{txt_path2}.txt\"\n",
    "        output_path = f\"{txt_path2}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{txt_path2}.txt\"\n",
    "        output_path = f\"{folder}/{txt_path2}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Appliquer les fonctions à leur corpus respectif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Output has been written in ../data/Data1_Le Drapeau Rouge_clean.txt!'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text (txt_path1, folder =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Output has been written in ../data/Data2_La libre Belgique_clean.txt!'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text (txt_path2, folder =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Data1_Le Drapeau Rouge\\\\../data/Data1_Le Drapeau Rouge_clean.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(txt_path1, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mtxt_path1\u001b[39m}\u001b[39;49;00m\u001b[39m_clean.txt\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     after \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m after[:\u001b[39m500\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Data1_Le Drapeau Rouge\\\\../data/Data1_Le Drapeau Rouge_clean.txt'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(txt_path1, f'{txt_path1}_clean.txt'), 'r', encoding= 'utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a4158a1d27bb34dbff8b699b1a83206a38e7c1399f365a62315d109c5919625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
